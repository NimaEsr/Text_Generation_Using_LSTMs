{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3653a2d",
   "metadata": {},
   "source": [
    "# Name Generation using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ac8d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import sys\n",
    "import unidecode\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90d72f9",
   "metadata": {},
   "source": [
    "Set the device to `cuda` if it is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68f06b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d800c",
   "metadata": {},
   "source": [
    "Let's get all characters from `string.printable`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76529407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The total number of characters : 100\n"
     ]
    }
   ],
   "source": [
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "print(f\" The total number of characters : {n_characters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed82fed8",
   "metadata": {},
   "source": [
    "Now it's time to load the data and read the large text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3657102",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = unidecode.unidecode(open(\"data/names.txt\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f10d5a6",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "\n",
    "We will use a Recurrent Neural Network (RNN) in this notebook. RNNs are popular and strong tools that can handle dynamic sequences of varying size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd73f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embed = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embed(x)\n",
    "        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n",
    "        out = self.fc(out.reshape(out.shape[0], -1))\n",
    "        return out, (hidden, cell)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ee1f81",
   "metadata": {},
   "source": [
    "Now Let's create our generator as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3704a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator():\n",
    "    def __init__(self, chunk_len, num_epchos, batch_size, hidden_size, \n",
    "                 num_layers, lr):\n",
    "        self.chunk_len = chunk_len\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.print_every = 50\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lr = lr\n",
    "\n",
    "\n",
    "    def char_tensor(self, string):\n",
    "        tensor = torch.zeros(len(string)).long()\n",
    "        for c in range(len(string)):\n",
    "            tensor[c] = all_characters.index(string[c])\n",
    "        return tensor\n",
    "\n",
    "    def get_random_batch(self):\n",
    "        start_idx = random.randint(0, len(file) - self.chunk_len)\n",
    "        end_idx = start_idx + self.chunk_len + 1\n",
    "        text_str = file[start_idx:end_idx]\n",
    "        text_input = torch.zeros(self.batch_size, self.chunk_len)\n",
    "        text_target = torch.zeros(self.batch_size, self.chunk_len)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            text_input[i, :] = self.char_tensor(text_str[:-1])\n",
    "            text_target[i, :] = self.char_tensor(text_str[1:])\n",
    "\n",
    "        return text_input.long(), text_target.long()\n",
    "\n",
    "    def generate(self, initial_str=\"A\", predict_len=100, temperature=0.85):\n",
    "        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n",
    "        initial_input = self.char_tensor(initial_str)\n",
    "        predicted = initial_str\n",
    "\n",
    "        for p in range(len(initial_str) - 1):\n",
    "            _, (hidden, cell) = self.rnn(\n",
    "                initial_input[p].view(1).to(device), hidden, cell\n",
    "            )\n",
    "\n",
    "        last_char = initial_input[-1]\n",
    "\n",
    "        for p in range(predict_len):\n",
    "            output, (hidden, cell) = self.rnn(\n",
    "                last_char.view(1).to(device), hidden, cell\n",
    "            )\n",
    "            output_dist = output.data.view(-1).div(temperature).exp()\n",
    "            top_char = torch.multinomial(output_dist, 1)[0]\n",
    "            predicted_char = all_characters[top_char]\n",
    "            predicted += predicted_char\n",
    "            last_char = self.char_tensor(predicted_char)\n",
    "\n",
    "        return predicted\n",
    "    \n",
    "        # input_size, hidden_size, num_layers, output_size\n",
    "    def train(self):\n",
    "        self.rnn = RNN(\n",
    "            n_characters, self.hidden_size, self.num_layers, n_characters\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n",
    "\n",
    "        print(\" Start Training ...\")\n",
    "\n",
    "        for epoch in range(1, self.num_epochs + 1):\n",
    "            inp, target = self.get_random_batch()\n",
    "            hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n",
    "\n",
    "            self.rnn.zero_grad()\n",
    "            loss = 0\n",
    "            inp = inp.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            for c in range(self.chunk_len):\n",
    "                output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n",
    "                loss += criterion(output, target[:, c])\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss = loss.item() / self.chunk_len\n",
    "\n",
    "            if epoch % self.print_every == 0:\n",
    "                print(f\"Epoch {epoch}/{num_epochs}, Loss: {loss}\")\n",
    "                print(self.generate())\n",
    "\n",
    "            writer.add_scalar(\"Training loss\", loss, global_step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "653d33de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Start Training ...\n",
      "Epoch 50/1000, Loss: 2.45411865234375\n",
      "Aley\n",
      "Taihnsa\n",
      "Larcie\n",
      "Kerreeli\n",
      "Jorannan\n",
      "Tarsere\n",
      "Latvina\n",
      "Mowisto\n",
      "Larli\n",
      "Mania\n",
      "Jeliccon\n",
      "Ketzyn\n",
      "Aaria\n",
      "Hisen\n",
      "Epoch 100/1000, Loss: 2.1676904296875\n",
      "Avie\n",
      "Sharia\n",
      "Rotha\n",
      "Jocharia\n",
      "Frian\n",
      "Damile\n",
      "Darida\n",
      "Oleto\n",
      "Mairbie\n",
      "Malla\n",
      "Nistte\n",
      "Alleisepha\n",
      "Pathie\n",
      "Carroni\n",
      "M\n",
      "Epoch 150/1000, Loss: 2.331116455078125\n",
      "Aley\n",
      "Blarina\n",
      "Josly\n",
      "Casler\n",
      "Jade\n",
      "Erine\n",
      "Lelary\n",
      "Nathar\n",
      "Tro\n",
      "Turia\n",
      "Kardelina\n",
      "Rorbine\n",
      "Jurit\n",
      "Dobrin\n",
      "Marenna\n",
      "R\n",
      "Epoch 200/1000, Loss: 1.981636474609375\n",
      "Alen\n",
      "Lolett\n",
      "Patty\n",
      "Keonell\n",
      "Ell\n",
      "Melony\n",
      "Bornin\n",
      "Mirgiage\n",
      "Glarce\n",
      "Dorron\n",
      "Josnin\n",
      "Stanton\n",
      "Tover\n",
      "Kerie\n",
      "Bennell\n",
      "Epoch 250/1000, Loss: 2.064339111328125\n",
      "Ara\n",
      "Gendra\n",
      "Erta\n",
      "Jeul\n",
      "Jamen\n",
      "Henavie\n",
      "Gorile\n",
      "Chard\n",
      "Claree\n",
      "Brece\n",
      "Latalda\n",
      "Jossa\n",
      "Athel\n",
      "Ferel\n",
      "Teris\n",
      "Jashele\n",
      "\n",
      "Epoch 300/1000, Loss: 1.9355018310546874\n",
      "Alie\n",
      "Jeya\n",
      "Gila\n",
      "Sharry\n",
      "Colly\n",
      "Leomine\n",
      "Airna\n",
      "Ryenton\n",
      "Noga\n",
      "Teahel\n",
      "Bian\n",
      "Artha\n",
      "Eesamue\n",
      "Gevi\n",
      "Tomin\n",
      "Belly\n",
      "Eri\n",
      "Epoch 350/1000, Loss: 1.951608642578125\n",
      "Amen\n",
      "Stamen\n",
      "Pekelisond\n",
      "Gilliela\n",
      "Joy\n",
      "Coosa\n",
      "Chacy\n",
      "Flonny\n",
      "Logey\n",
      "Bex\n",
      "Trise\n",
      "Deffferip\n",
      "Kelliso\n",
      "Lona\n",
      "Deanna\n",
      "\n",
      "Epoch 400/1000, Loss: 2.0442003173828125\n",
      "Ases\n",
      "Layson\n",
      "Lona\n",
      "Martino\n",
      "Randre\n",
      "Dena\n",
      "Kathrynn\n",
      "Kettine\n",
      "Kara\n",
      "Adilyn\n",
      "Shandroe\n",
      "Ianny\n",
      "Amil\n",
      "Amon\n",
      "Evand\n",
      "Clac\n",
      "Epoch 450/1000, Loss: 1.924220947265625\n",
      "Addie\n",
      "Ronique\n",
      "Vaton\n",
      "Royen\n",
      "Amella\n",
      "Mikley\n",
      "Katherici\n",
      "Stanne\n",
      "Janie\n",
      "Dora\n",
      "Kathler\n",
      "Walley\n",
      "Kathien\n",
      "Buod\n",
      "Hatri\n",
      "Epoch 500/1000, Loss: 1.6437225341796875\n",
      "AnnasAnna\n",
      "Isabel\n",
      "Elex\n",
      "Kaylen\n",
      "Erick\n",
      "Raccay\n",
      "Cora\n",
      "Shabel\n",
      "Joss\n",
      "Marinie\n",
      "Dally\n",
      "Estin\n",
      "Edsie\n",
      "Seven\n",
      "Pacie\n",
      "Kris\n",
      "Epoch 550/1000, Loss: 1.6726871337890625\n",
      "Alleen\n",
      "Deeralie\n",
      "Alianne\n",
      "Channette\n",
      "Pristine\n",
      "Sara\n",
      "Vackeris\n",
      "Wyllie\n",
      "Kathy\n",
      "Lean\n",
      "Harley\n",
      "Mariel\n",
      "Aniza\n",
      "Goma\n",
      "C\n",
      "Epoch 600/1000, Loss: 1.8347279052734375\n",
      "Aleigh\n",
      "Zasaina\n",
      "Dayna\n",
      "Jamino\n",
      "Collynn\n",
      "Alvin\n",
      "Rianna\n",
      "Casian\n",
      "Joke\n",
      "Joel\n",
      "Edney\n",
      "Kebdy\n",
      "Tonitia\n",
      "Ashtony\n",
      "Maryann\n",
      "Epoch 650/1000, Loss: 2.3603173828125\n",
      "Ardin\n",
      "Allanda\n",
      "Niona\n",
      "Pristel\n",
      "Zeannon\n",
      "Mara\n",
      "Beon\n",
      "Judit\n",
      "Karle\n",
      "Susan\n",
      "Tathamine\n",
      "Noil\n",
      "Daney\n",
      "Yachello\n",
      "Miscy\n",
      "B\n",
      "Epoch 700/1000, Loss: 1.708624267578125\n",
      "Alan\n",
      "Possie\n",
      "Jake\n",
      "Alexian\n",
      "Shaina\n",
      "Andreus\n",
      "Rushan\n",
      "Arsus\n",
      "Lolyna\n",
      "Loone\n",
      "Carley\n",
      "Rose\n",
      "Jean\n",
      "Lewinia\n",
      "Leorance\n",
      "R\n",
      "Epoch 750/1000, Loss: 1.6276287841796875\n",
      "Alance\n",
      "Morson\n",
      "Eja\n",
      "Mirger\n",
      "Cecilia\n",
      "Emmen\n",
      "andon\n",
      "Kristie\n",
      "Shilly\n",
      "Jose\n",
      "Galiza\n",
      "Bathew\n",
      "Frider\n",
      "Ilaena\n",
      "Alfres\n",
      "E\n",
      "Epoch 800/1000, Loss: 2.03237255859375\n",
      "Alac\n",
      "Aril\n",
      "Gorgert\n",
      "Cristor\n",
      "Isaba\n",
      "Christopor\n",
      "Morquidas\n",
      "Savana\n",
      "Mairane\n",
      "Keil\n",
      "Keckei\n",
      "Burt\n",
      "Dennis\n",
      "Kyler\n",
      "Lor\n",
      "Epoch 850/1000, Loss: 1.5489093017578126\n",
      "Alaa\n",
      "Deresha\n",
      "Shanna\n",
      "Korin\n",
      "Sandy\n",
      "Erthur\n",
      "Kian\n",
      "Deod\n",
      "Heromany\n",
      "Kendy\n",
      "Calia\n",
      "Robert\n",
      "Joseph\n",
      "Christina\n",
      "Asay\n",
      "To\n",
      "Epoch 900/1000, Loss: 1.5685303955078125\n",
      "Annah\n",
      "Arian\n",
      "Kidney\n",
      "Cary\n",
      "Jorine\n",
      "Joe\n",
      "Rulore\n",
      "Dara\n",
      "Tonietta\n",
      "Jose\n",
      "Julius\n",
      "Treelah\n",
      "Sherrie\n",
      "Larsha\n",
      "Laur\n",
      "Rose\n",
      "\n",
      "Epoch 950/1000, Loss: 1.448444091796875\n",
      "Aleen\n",
      "Ashma\n",
      "Shaniya\n",
      "Isa\n",
      "Alex\n",
      "Ellen\n",
      "Geolly\n",
      "Karrell\n",
      "Essella\n",
      "Ellen\n",
      "Gianalda\n",
      "Lucol\n",
      "Alecia\n",
      "Arielle\n",
      "Conner\n",
      "\n",
      "Epoch 1000/1000, Loss: 1.371662109375\n",
      "Athen\n",
      "Alanuel\n",
      "Ariana\n",
      "Rosemari\n",
      "Aige\n",
      "Emili\n",
      "Marco\n",
      "Berthenia\n",
      "Eloifer\n",
      "Alisandre\n",
      "Chels\n",
      "Gretsa\n",
      "Charlene\n",
      "Eddi\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 250\n",
    "num_epochs = 1000\n",
    "batch_size = 1\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "lr = 0.005\n",
    "initial_str = \"A\"\n",
    "predict_len = 100\n",
    "\n",
    "gennames = Generator(chunk_len, num_epochs, batch_size, hidden_size, \n",
    "                     num_layers, lr)\n",
    "gennames.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6913de1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
